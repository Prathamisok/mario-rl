{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_W1vmZrNbI7a",
        "outputId": "a85dd05e-94ff-4ebc-aff6-65534b03cc0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym-super-mario-bros in /usr/local/lib/python3.7/dist-packages (7.4.0)\n",
            "Requirement already satisfied: nes-py>=8.1.4 in /usr/local/lib/python3.7/dist-packages (from gym-super-mario-bros) (8.2.1)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.21.6)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (1.5.0)\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.7/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros) (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.21,>=1.4.0->nes-py>=8.1.4->gym-super-mario-bros) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym-super-mario-bros"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import RIGHT_ONLY\n",
        "import gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import pickle \n",
        "import numpy as np\n",
        "import collections \n",
        "import cv2\n",
        "import time\n",
        "import statistics\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch.nn.functional as nnf\n",
        "print(torch.cuda.device_count())\n",
        "device = torch.device('cuda:0')\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAL00vcGcjZm",
        "outputId": "f4cfbafc-cd47-470d-b0fb-0e71250510fb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SkipAndMax(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"\n",
        "          1. Take the same action for 'skip' number of obs\n",
        "          2. Return the max of 2 most recent obs\n",
        "        \"\"\"\n",
        "        super(SkipAndMax, self).__init__(env)\n",
        "        self.obs_buffer = collections.deque(maxlen=2) # 2 most recent observations\n",
        "        self._skip = skip # Use the same action for 'skip' number of obs\n",
        "\n",
        "    def step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self.obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Take max of 2 most recent obs\n",
        "        max_frame = np.max(np.stack(self.obs_buffer), axis=0) \n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.obs_buffer.clear() # Clear obs buffer\n",
        "        obs = self.env.reset()\n",
        "        self.obs_buffer.append(obs) # Append initial obs\n",
        "        return obs\n",
        "\n",
        "\n",
        "class Frame_Processing(gym.ObservationWrapper):\n",
        "    \"\"\"\n",
        "      Downsamples to 84x84 => Greyscales\n",
        "    \"\"\"\n",
        "    def __init__(self, env=None):\n",
        "        super(Frame_Processing, self).__init__(env)\n",
        "        old_shape = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8).shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n",
        "                                                dtype=np.float32)\n",
        "    def observation(self, obs):\n",
        "        obs = Frame_Processing.process(obs)\n",
        "        return np.moveaxis(obs, 2, 0) # Image to Pytorch\n",
        "\n",
        "    @staticmethod\n",
        "    def process(frame):\n",
        "      \n",
        "        if frame.size == 240 * 256 * 3:\n",
        "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
        "        else:\n",
        "            assert False, \"Wrong resolution.\"\n",
        "\n",
        "        # Converting a colored RGB image into a grayscale image\n",
        "        # Formula: Y = 0.299R + 0.587G + 0.114B\n",
        "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "        \n",
        "        # Resize to 84*84\n",
        "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
        "        x_t = resized_screen[18:102, :]\n",
        "        x_t = np.reshape(x_t, [84, 84, 1])\n",
        "        return x_t.astype(np.uint8)\n",
        "\n",
        "class BufferingWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps, dtype=np.float32):\n",
        "        super(BufferingWrapper, self).__init__(env)\n",
        "        self.dtype = dtype\n",
        "        old_space = env.observation_space\n",
        "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
        "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
        "\n",
        "    def reset(self):\n",
        "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
        "        return self.observation(self.env.reset())\n",
        "\n",
        "    def observation(self, observation):\n",
        "        self.buffer[:-1] = self.buffer[1:]\n",
        "        self.buffer[-1] = observation\n",
        "        return np.array(self.buffer).astype(np.float32) / 255.0 # Normalize pixel values in frame --> 0 to 1"
      ],
      "metadata": {
        "id": "r8N5E8WDcjcd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MarioBrossEnvironment:\n",
        "  def __init__(self, skip = 4, buffer_step = 4):\n",
        "      self.env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
        "      self.env = SkipAndMax(self.env)\n",
        "      self.env = Frame_Processing(self.env)\n",
        "      self.env = BufferingWrapper(self.env, 4)\n",
        "      self.env = JoypadSpace(self.env, RIGHT_ONLY) \n",
        "\n",
        "      self.env.seed(500)\n",
        "      self.nb_step = 0\n",
        "      self.observation_space_final = self.env.observation_space.shape\n",
        "      self.action_space = self.env.action_space.n\n",
        "  \n",
        "  def sample_action(self):\n",
        "        return self.env.action_space.sample()\n",
        "\n",
        "  def reset(self):\n",
        "      self.nb_step = 0\n",
        "      state = self.env.reset()\n",
        "      state = torch.unsqueeze(torch.tensor(state),0)\n",
        "      return state\n",
        "\n",
        "  def step(self,action):\n",
        "      state, reward, done, info = self.env.step(action)\n",
        "      state = torch.unsqueeze(torch.tensor(state),0)\n",
        "      return state, reward, done, info\n",
        "\n",
        "  def render(self):\n",
        "        self.env.render()\n",
        "\n",
        "  def close(self):\n",
        "        self.env.close()\n",
        "\n",
        "  def show_state(self,info=\"\"):\n",
        "        plt.figure(3)\n",
        "        plt.clf()\n",
        "        plt.imshow(self.env.render(mode='rgb_array'))\n",
        "        plt.title(\"Episode: %d %s\" % (self.nb_step, info))\n",
        "        plt.axis('off')\n",
        "\n",
        "        display.clear_output(wait=True)\n",
        "        display.display(plt.gcf())"
      ],
      "metadata": {
        "id": "LPh8YqEqcjfZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classic_ConvNet(nn.Module):\n",
        "  def __init__(self, input_size, n_actions):\n",
        "        super(Classic_ConvNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = n_actions\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_size[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        conv_out_size = self.getShapeAfterConvolutions(self.input_size)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.output_size)\n",
        "        )\n",
        "    \n",
        "  def getShapeAfterConvolutions(self, shape):\n",
        "      o = self.conv(torch.zeros(1, *shape))\n",
        "      return int(np.prod(o.size()))\n",
        "\n",
        "  def forward(self, x):\n",
        "      conv_out = self.conv(x)\n",
        "      conv_out=conv_out.view(x.size()[0], -1)\n",
        "      return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "2USeHyepc07E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Deeper_ConvNet(nn.Module):\n",
        "  def __init__(self, input_size, n_actions):\n",
        "        super(Deeper_ConvNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = n_actions\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_size[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        conv_out_size = self.getShapeAfterConvolutions(self.input_size)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.output_size)\n",
        "        )\n",
        "    \n",
        "  def getShapeAfterConvolutions(self, shape):\n",
        "      o = self.conv(torch.zeros(1, *shape))\n",
        "      return int(np.prod(o.size()))\n",
        "\n",
        "  def forward(self, x):\n",
        "      conv_out = self.conv(x)\n",
        "      conv_out=conv_out.view(x.size()[0], -1)\n",
        "      return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "HnHe6Fvoc0-h"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Wider_ConvNet(nn.Module):\n",
        "  def __init__(self, input_size, n_actions):\n",
        "        super(Wider_ConvNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = n_actions\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_size[0], 64, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        conv_out_size = self.getShapeAfterConvolutions(self.input_size)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, self.output_size)\n",
        "        )\n",
        "    \n",
        "  def getShapeAfterConvolutions(self, shape):\n",
        "      o = self.conv(torch.zeros(1, *shape))\n",
        "      return int(np.prod(o.size()))\n",
        "\n",
        "  def forward(self, x):\n",
        "      conv_out = self.conv(x)\n",
        "      conv_out=conv_out.view(x.size()[0], -1)\n",
        "      return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "TP0a3Fq2c1B8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxPoolingClassic_ConvNet(nn.Module):\n",
        "  def __init__(self, input_size, n_actions):\n",
        "        super(MaxPoolingClassic_ConvNet, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.output_size = n_actions\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(self.input_size[0], 32, kernel_size=8, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        \n",
        "        conv_out_size = self.getShapeAfterConvolutions(self.input_size)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, self.output_size)\n",
        "        )\n",
        "    \n",
        "  def getShapeAfterConvolutions(self, shape):\n",
        "      o = self.conv(torch.zeros(1, *shape))\n",
        "      return int(np.prod(o.size()))\n",
        "\n",
        "  def forward(self, x):\n",
        "      conv_out = self.conv(x)\n",
        "      conv_out=conv_out.view(x.size()[0], -1)\n",
        "      return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "K-lGz-YFcjim"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, \n",
        "                 model_base,  # resnet backbone\n",
        "                 input_size,\n",
        "                 n_actions,\n",
        "                 trainable_features=True # whether training the feature extractor\n",
        "                 ):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.output_size = n_actions\n",
        "\n",
        "        self.trainable_features = trainable_features\n",
        "\n",
        "        # Initial conv\n",
        "        self.conv_home = nn.Conv2d(self.input_size[0], 3, kernel_size=1, stride=1)\n",
        "        \n",
        "        # For colors, see the picture of resnet\n",
        "        # Orange \n",
        "        self.conv1 = model_base.conv1\n",
        "        self.bn1 = model_base.bn1\n",
        "        self.relu = model_base.relu\n",
        "        self.maxpool = model_base.maxpool\n",
        "\n",
        "        # Purple\n",
        "        self.layer1 = model_base.layer1\n",
        "        \n",
        "        # Green\n",
        "        self.layer2 = model_base.layer2\n",
        "\n",
        "        # Red \n",
        "        self.layer3 = model_base.layer3\n",
        "\n",
        "        # Blue\n",
        "        self.layer4 = model_base.layer4\n",
        "        \n",
        "\n",
        "        # Grey\n",
        "        self.avgpool = model_base.avgpool\n",
        "\n",
        "        # Feature extractor\n",
        "        self.feature_layers = nn.Sequential(self.conv_home, self.conv1, self.bn1, self.relu, self.maxpool, \n",
        "                                            self.layer1, self.layer2, self.layer3, self.layer4, \n",
        "                                            self.avgpool)\n",
        "        \n",
        "        self.fc = nn.Linear(model_base.fc.in_features, self.output_size)\n",
        "        self.__in_features = model_base.fc.in_features\n",
        "\n",
        "    def forward(self, x): # TODO\n",
        "        features = self.feature_layers(x) # TODO\n",
        "        features = features.view(features.size(0), -1)\n",
        "        y = self.fc(features) # TODO\n",
        "        return y"
      ],
      "metadata": {
        "id": "HkeL3ZrKc__J"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepQNetworkAgent:\n",
        "  def __init__(self, model,env, double, resnet_pretrained = None, test_epoch = 50):\n",
        "    \n",
        "    # Environment attributes\n",
        "    self.env=env\n",
        "    self.num_actions = self.env.action_space\n",
        "    self.state_shape = self.env.observation_space_final\n",
        "    self.test_epoch = test_epoch\n",
        "    \n",
        "    # Agent parameters\n",
        "    self.gamma = None\n",
        "    self.eps = None\n",
        "    self.eps_min = None\n",
        "    self.eps_decay = None\n",
        "    self.double = double\n",
        "    self.end_position = 0\n",
        "\n",
        "    # Double Neural Networks\n",
        "    if resnet_pretrained == None:\n",
        "      self.online_net = model(self.state_shape,self.num_actions)\n",
        "      self.target_net = model(self.state_shape,self.num_actions)\n",
        "    else:\n",
        "      self.online_net = model(resnet_pretrained,self.state_shape,self.num_actions)\n",
        "      self.target_net = model(resnet_pretrained,self.state_shape,self.num_actions)\n",
        "    \n",
        "    # Neural Networks parameters  \n",
        "    self.optimizer = None\n",
        "    self.loss = None\n",
        "    self.cuda = False\n",
        "    self.train_loader = None\n",
        "    self.weight_decay = None\n",
        "    self.lr = None\n",
        "    self.dropout = None\n",
        "\n",
        "    # Experiences memory\n",
        "    self.memory_size = None\n",
        "    self.mem_element = 0\n",
        "    self.mem_state = None\n",
        "    self.mem_nextstate = None\n",
        "    self.mem_reward = None\n",
        "    self.mem_action = None\n",
        "    self.mem_done = None\n",
        "    self.batch_size = None   \n",
        "\n",
        "  def compile(self, optimizer, loss, \n",
        "              gamma, memory_size, batch_size, eps_max, eps_min, eps_decay, copy_model , \n",
        "              dropout = 0.05,lr=0.001, weight_decay=0.001, cuda=True):\n",
        "    self.loss = loss()\n",
        "    self.gamma = gamma\n",
        "    self.memory_size = memory_size\n",
        "    self.batch_size = batch_size   \n",
        "    self.eps = eps_max\n",
        "    self.eps_max = eps_max\n",
        "    self.eps_min = eps_min\n",
        "    self.eps_decay = eps_decay\n",
        "    self.copy_model = copy_model\n",
        "    self.total_iter = 0\n",
        "    self.dropout = dropout\n",
        "    self.weight_decay = weight_decay\n",
        "    self.lr = lr\n",
        "    self.cuda = cuda\n",
        "\n",
        "    # Initialize the memory\n",
        "    self.mem_state = torch.zeros(self.memory_size, *self.state_shape).float()\n",
        "    self.mem_nextstate = torch.zeros(self.memory_size, *self.state_shape).float()\n",
        "    self.mem_reward = torch.zeros(self.memory_size, 1).float()\n",
        "    self.mem_action = torch.zeros(self.memory_size, 1).float()\n",
        "    self.mem_done = torch.zeros(self.memory_size, 1).float()\n",
        "\n",
        "    try: \n",
        "      self.optimizer = optimizer(params=self.online_net.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    except:\n",
        "      print('Requires the creation of a model')    \n",
        "\n",
        "    if self.cuda:\n",
        "      self.online_net = self.online_net.cuda()\n",
        "      self.target_net = self.target_net.cuda()  \n",
        "\n",
        "  def act(self, state):\n",
        "    if self.cuda:\n",
        "      state = state.cuda().float()\n",
        "    else:\n",
        "      state = state.float()\n",
        "\n",
        "    # Choose exploration or exploitation\n",
        "    if random.random() < self.eps:\n",
        "      #Exploration\n",
        "      return random.randint(0,self.num_actions-1)\n",
        "    # Exploitation\n",
        "    if self.double:\n",
        "      return torch.argmax(self.online_net(state)).item()\n",
        "    else:\n",
        "      return torch.argmax(self.target_net(state)).item()\n",
        "\n",
        "  def train_step(self,state_batch,nextstate_batch,reward_batch,\n",
        "                 action_batch,done_batch):\n",
        "    \n",
        "    self.optimizer.zero_grad()\n",
        "    ## Use GPU\n",
        "    if self.cuda:\n",
        "      state_batch = state_batch.cuda()\n",
        "      nextstate_batch = nextstate_batch.cuda()\n",
        "      reward_batch = reward_batch.cuda()\n",
        "      action_batch = action_batch.cuda()\n",
        "      done_batch = done_batch.cuda()\n",
        "    \n",
        "    ## Double DQN Learning: decouple selection with online_net and evaluation with target_net\n",
        "    if self.double:\n",
        "      selection = torch.argmax(self.online_net(nextstate_batch), dim = 1).unsqueeze(1)\n",
        "      evaluation = self.target_net(nextstate_batch).gather(1, selection.long())\n",
        "      target = reward_batch + torch.mul(self.gamma *  evaluation, 1 - done_batch).float()\n",
        "    ## DQN Learning: all is calculated with target_net\n",
        "    else:\n",
        "      target = reward_batch + torch.mul((self.gamma * \n",
        "                  self.target_net(nextstate_batch).max(1).values.unsqueeze(1)), \n",
        "                  1 - done_batch).float()\n",
        "    \n",
        "    actual = self.online_net(state_batch).gather(1, action_batch.long()).float() \n",
        "    # calculate the loss of the target (new rewards) and actual (calculated with online net)\n",
        "    loss = self.loss(target , actual)\n",
        "    loss.backward()\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def experience_replay(self):\n",
        "    # Experience replay:  get a sample from all the memory and train\n",
        "    min_sample = min(self.batch_size,self.mem_element)\n",
        "    sample_batch = random.choices(range(self.mem_element), k=min_sample)\n",
        "    state_batch = self.mem_state[sample_batch]\n",
        "    nextstate_batch = self.mem_nextstate[sample_batch]\n",
        "    reward_batch = self.mem_reward[sample_batch]\n",
        "    action_batch = self.mem_action[sample_batch]\n",
        "    done_batch = self.mem_done[sample_batch]\n",
        "\n",
        "    self.train_step(state_batch,nextstate_batch,reward_batch,\n",
        "                 action_batch,done_batch)\n",
        "\n",
        "  def update(self,state,action , nextstate, reward,done):\n",
        "\n",
        "    ## Update memory with new values\n",
        "    self.mem_state[self.end_position] = state.float()\n",
        "    self.mem_nextstate[self.end_position] = nextstate.float()\n",
        "    self.mem_reward[self.end_position] = torch.tensor([reward]).unsqueeze(0).float()\n",
        "    self.mem_action[self.end_position] = torch.tensor([action]).unsqueeze(0).float()\n",
        "    self.mem_done[self.end_position] = torch.tensor([done]).unsqueeze(0).float()\n",
        "\n",
        "    self.end_position = (self.end_position + 1) % self.memory_size  # FIFO tensor\n",
        "    self.mem_element = min(self.mem_element + 1, self.memory_size)\n",
        "\n",
        "\n",
        "    # Update the target_net with the parameters of the online_net\n",
        "    if self.total_iter % self.copy_model == 0:\n",
        "            self.transferParameters()\n",
        "    #  Train the model with experience replay\n",
        "    if self.mem_element>=self.batch_size:\n",
        "      self.experience_replay()\n",
        "    \n",
        "    self.total_iter+=1\n",
        "\n",
        "  def run(self, epochs, max_iter = 500):\n",
        "    #List of rewards per epoch\n",
        "    list_rewards = []\n",
        "    list_test_rewards = []\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      time_act = 0\n",
        "      time_get_env = 0\n",
        "      time_update = 0\n",
        "      time_total = 0\n",
        "      state = self.env.reset()\n",
        "      epoch_reward = 0\n",
        "      iter = 0\n",
        "      while True:\n",
        "        start_0 = time.time()\n",
        "        # select best action\n",
        "        action = self.act(state)\n",
        "        time_act += time.time() - start_0\n",
        "        start_1 = time.time()\n",
        "        # Apply action and get new state and rewards\n",
        "        nextstate, reward, done, info = self.env.step(action)\n",
        "        time_get_env += time.time() - start_1\n",
        "        start_2 = time.time()\n",
        "        # Update the model with the last interaction\n",
        "        self.update(state,action , nextstate, reward,done)\n",
        "        time_update += time.time() - start_2\n",
        "        # Move to the next state\n",
        "        state = torch.tensor(nextstate)\n",
        "        epoch_reward+=reward\n",
        "        iter+=1\n",
        "        time_total += time.time() - start_0\n",
        "        # If Mario died or we won, finish the epoch\n",
        "        if done or iter>=max_iter:\n",
        "          break\n",
        "\n",
        "      # Decay the eps every 2 epochs\n",
        "      if epoch>0 and epoch%2 == 0:\n",
        "        self.eps *= self.eps_decay\n",
        "        self.eps = max(self.eps, self.eps_min)\n",
        "        print(\"eps = \", self.eps)\n",
        "\n",
        "      # Test the performance of the net each test_epoch (without exploration)\n",
        "      if epoch>0 and epoch%self.test_epoch == 0:\n",
        "        test_rewards = self.test(4,4000,False)\n",
        "        list_test_rewards.append(test_rewards)\n",
        "\n",
        "      list_rewards.append(epoch_reward)\n",
        "      print(\"\")\n",
        "      print(\"Reward: \", epoch_reward)\n",
        "      #print(\"Avg_time_getenv: \", time_get_env)\n",
        "      #print(\"Avg_time_update: \", time_update)\n",
        "      #print(\"Avg_time_total: \", time_total)\n",
        "\n",
        "    return (list_rewards,list_test_rewards)\n",
        "\n",
        "  def transferParameters(self):\n",
        "    self.target_net.load_state_dict(self.online_net.state_dict())\n",
        "\n",
        "  def test(self, epochs = 50, maxiter = 1000, show = False, eps_test = 0.01):\n",
        "    # Set exploration eps as 0 to evaluate only the model performance\n",
        "    eps_old = self.eps\n",
        "    self.eps = eps_test\n",
        "    list_rewards = []\n",
        "    done = True\n",
        "    for ep in range(epochs):\n",
        "      epoch_reward = 0\n",
        "      for step in range(maxiter):\n",
        "          if done:\n",
        "              state = self.env.reset()\n",
        "          action = self.act(state)\n",
        "          nextstate, reward, done, info = self.env.step(action)\n",
        "          epoch_reward+=reward\n",
        "          state = torch.tensor(nextstate)\n",
        "          if show:\n",
        "            self.env.show_state()\n",
        "          if done:\n",
        "            break\n",
        "      list_rewards.append(epoch_reward)\n",
        "    self.eps = eps_old\n",
        "    return list_rewards\n",
        "\n",
        "  # Save model to reuse it later\n",
        "  def saveParameters(self, path):\n",
        "    torch.save(self.online_net.state_dict(), path)\n",
        "    \n",
        "  # Load parameters from a pretrained model.\n",
        "  def pretrainedParameters(self, path):\n",
        "    self.online_net.load_state_dict(torch.load(path))\n",
        "    self.target_net.load_state_dict(torch.load(path))"
      ],
      "metadata": {
        "id": "46wsxwJVdAFU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mario = MarioBrossEnvironment()\n",
        "agent = DeepQNetworkAgent(Classic_ConvNet, mario, double = True)\n",
        "agent.compile( optimizer = torch.optim.Adam, loss = nn.SmoothL1Loss, \n",
        "              gamma = 0.9, memory_size = 10000, batch_size = 32, \n",
        "              eps_max = 1, eps_min = 0.02, lr = 0.00025, eps_decay = 0.99, copy_model = 5000)\n",
        "## The agent learns here!!\n",
        "rewards,test_rewards_run = agent.run(5000, 4000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LuM3pCsdD3G",
        "outputId": "5467896a-18f9-4424-c2da-7d47029e4c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:185: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  0%|          | 1/5000 [00:04<6:34:32,  4.74s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Reward:  640.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_doubleDQN = \"DoubleDQN_Classic\"\n",
        "doubleDQN_pd = pd.read_csv('DoubleDQN.csv',header = None)\n",
        "pd_data = doubleDQN_pd\n",
        "pd_data.columns = [name_doubleDQN]\n",
        "\n",
        "pd_data = pd_data.reset_index()\n",
        "bines = [(i) * 50 for i in list(range(101))]\n",
        "bines_lab = [(i) * 50 for i in list(range(100))]\n",
        "pd_data['binned'] = pd.cut(pd_data['index'], bins=bines, labels=bines_lab)\n",
        "pd_data = pd_data.fillna(0)\n",
        "datag = pd_data.groupby(['binned']).mean()\n",
        "datag = datag.reset_index()\n",
        "dataq_min = pd_data.groupby(['binned']).quantile(.10)\n",
        "dataq_min = dataq_min.reset_index()\n",
        "dataq_max = pd_data.groupby(['binned']).quantile(.90)\n",
        "dataq_max = dataq_max.reset_index()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.array(datag['binned']),np.array(datag[name_doubleDQN]))\n",
        "ax.fill_between(datag['binned'], dataq_min[name_doubleDQN], dataq_max[name_doubleDQN], alpha=.3)\n",
        "\n",
        "ax.set_xlabel(\"Training episodes\")\n",
        "ax.set_ylabel(\"Reward\")\n",
        "\n",
        "ax.set_title('Double DQN rewards', fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LjiFPJtHdD6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_rewards = agent.test(50,4000,False, eps_test = 0)\n",
        "agent.test(1,4000,True)\n",
        "print(\"Average reward: \", statistics.mean(test_rewards))"
      ],
      "metadata": {
        "id": "kxAGa0DodD94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
